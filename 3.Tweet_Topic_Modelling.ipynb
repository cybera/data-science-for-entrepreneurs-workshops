{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis With Twitter Data\n",
    "\n",
    "So far in this tutorial series we've discussed at length how to clean and process Twitter data. We've also introduced aspects of Natural Language Processing (NLP) such as word clouds and counting statistics. In this notebook, we're going to construct a topic model using **word vectors**, **word embeddings** and **K-means**. Using these methods, we will show you a few tricks in order to identify groups of tweets defining topics you're interested in. Such a model has applications for the identification of groups of users for targeted advertising,  or as a tool to quickly find relevant tweets to help quantify potential demand for a product/service. \n",
    "\n",
    "## Goals\n",
    "1. Turn our tweets into \"tweet\" vectors. \n",
    "2. Cluster those vectors to define topics.\n",
    "3. Use this model to locate topics relevant to us, and discuss how such a model may be used for market research or advertising.\n",
    "\n",
    "# Preliminaries \n",
    "Before we get started, let's introduce a few concepts and background materials before diving into the materials. For this notebook we have two main concepts to cover: Word vectors, word embeddings. The introduction into k-means clustering will be saved for the relevant section.\n",
    "\n",
    "## Word vectors\n",
    "\n",
    "A word vector is the conversion of a word into a sparse 'one-hot' coded vector. What this means is that each word contained within our tweets is mapped to a unique vector with only a single non-zero element. For example, the word \"karate\" may appear as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{Karate} = \n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Where it is represented as a _unique_ and _sparse_ vector that contains a single non-zero entry. The size of each word vector is the same and have size $N$, where $N$ is the number of words in our vocabulary. \n",
    "\n",
    "## Sentence Vectors\n",
    "\n",
    "A sentence vector is simply the word vector representation of sentence or in our case, a tweet. For example, the sentence\n",
    "``` \n",
    "wax on, wax off\n",
    "```\n",
    "\n",
    "is the average of the vector sum of our word vectors for each word\n",
    "\\begin{equation}\n",
    "\\vec{D}_1 = \\frac{1}{3} \\left(2\\vec{w}_1 + \\vec{w}_2 + \\vec{w_3}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Notice how the vector for `wax` was repeated in this particular example. What this means is that in sentence vectors **punctuation, and the order words appear is not important**. The vector representation of a sentence is the same. For example, the sentences\n",
    "\n",
    "```\n",
    "Turning words to math? That is lunacy! Why would you do that? \n",
    "```\n",
    "and\n",
    "```\n",
    "Turning math is lunacy! Why would you do that to words?\n",
    "```\n",
    "\n",
    "would have an identical _vector_ representation, but are very clearly different sentences. \n",
    "\n",
    "## Word Embeddings \n",
    "\n",
    "On their own, word vectors are often too large and sparse to be particularly useful. More importantly however, because word vectors are linearly independent (no combination of word vectors can be used to make a different word vector) understanding similarities between words (such as synonyms) and the similarities between sentences is impossible. How do we transform these vectors into a form that can be used to understand linguistic similarities? \n",
    "\n",
    "What we need to do is develop our word embeddings - a new set of vectors which can relate words and sentences based on their similarities. In our case, we're going to use what's known as a \"Neural Embedding\", or word embeddings that are a result of a neural network trained on text data. In most applications, a fully connected shallow neural network (A neural network with only a single hidden layer) is used, and the resulting parameter weights of the neural network define our _word embeddings_ or a series of **dense** vectors that represent the similarities between words. For our purposes we will be using a set of pre-trained word embeddings available [in this github repository](https://github.com/eyaler/word2vec-slim/blob/master/GoogleNews-vectors-negative300-SLIM.bin.gz). This is a small version of word embeddings resulting from training on Google News, and has a vocabulary of about 300000 words. \n",
    "\n",
    "Understanding of how these word embeddings is not required. However, for those interested in learning more, feel free to use the drop down menus below in order to understand how the neural network is used to create word embeddings. The first drop down is very high level, and the next two are quite technical. \n",
    "\n",
    "---\n",
    "<br> \n",
    "## Would you like to know more? \n",
    "<details>\n",
    "<summary> Click here to learn about the idea of a word embedding </summary>\n",
    "<br>\n",
    "<h2> Word2Vec Word Embeddings </h2>\n",
    " \n",
    "In this section we will cover the basics of the model known as `word2vec` using what is known as the Continuous Bag Of Words (CBOW) model. In the CBOW model, our task is to predict \"missing\" words. For example, if we had the sentence \n",
    "```\n",
    "I sure do love learning about word embeddings! \n",
    "```\n",
    "\n",
    "To train our network, we would remove a single word from the sentence above, say \"learning\". We would then and predict the word \"learning\" given an input of the other words in the sentence. Schematically this is represented below \n",
    "\n",
    "![scheme](images/wor2vecschem.png)\n",
    "\n",
    "In this case, we take the input context - the other words in our original sentence, and use those in order to predict the word that we've removed.  As words can exist in multiple contexts (among different sets of input words), this model can discover how different words are related to each other - with enough data we'll understand the \"relationships\" between words. What is nice about this model is that we have a massive _labeled_ data set for free: all we need to do is grab a bunch of text, and choose which word to move at each step of training!\n",
    "\n",
    "But wait! At face value, this neural network doesn't sound like it's particularly useful - all its doing is predicting a word based on some words we've given to it as input, we can't even use that outside of the texts we've given it! How is this network useful for understanding language? Well, we won't be using the neural network in the sense that we're using it \"online\" to create word embeddings. What we'll have from the training process is a set of internal parameters that relate our one-hot encoded vector to _all the other_ one hot encoded vectors. The closer these vectors of internal parameters are together, the more similar words are to each other. In the case of word embeddings, the network we use to create them is _only_ useful in terms of the weights it optimizes - not the output it produces.\n",
    "\n",
    "What this means is that we will have word embeddings that represent the contextual similarities between words. Instead of using our simple one hot encoded vectors, we will now possess a vector formalism that can be used to understand things such as word similarity, and expand those to things like sentence and document similarity. \n",
    "\n",
    "---\n",
    "\n",
    "## Would you like to know more? \n",
    "<details>\n",
    "<summary> Click here to learn more about the neural network and the mathematics </summary>\n",
    "<br>\n",
    "<h2> Word2vec neural network and mathematics </h2>\n",
    "\n",
    "The `word2vec` neural network appears most often as the following schematic\n",
    "\n",
    "> ![word2vec](images/word2vec.png)\n",
    "> \n",
    "> Image source: https://arxiv.org/pdf/1411.2738.pdf\n",
    "\n",
    "A brief note on notation: bold quantities will represent both vectors and matrices, and we hope it will be clear given context. In the image above, our input layer takes a word vector $\\mathbf{x}$, which is the sentence vector with our target word removed (this vector is referred to as the \"context\"), and $\\mathbf{y}$ is the one-hot word encoded vector that we removed from our sentence vector that we're trying to predict. These vectors are both of length $V$, the number of words in our vocabulary. \n",
    "\n",
    "We also have $N$ neurons in a hidden layer of $\\mathbf{h}$, and two weight matricies $\\mathbf{W}$ and $\\mathbf{W^\\prime}$. Here, $\\mathbf{W}$ is a $V \\times N$ matrix, and $\\mathbf{W^\\prime}$ is a $N \\times V$ matrix. Notice how the number of hidden neurons defines the size of our hidden parameter matrices. \n",
    "\n",
    "## Forward Propagation \n",
    "\n",
    "Given the schematic above, we see that the values the hidden layer's output is defined as \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\tag{1}\n",
    "\\label{eq:h}\n",
    "\\mathbf{h} = \\mathbf{W} \\mathbf{x} = \\frac{1}{C} \\mathbf{W} \\cdot \\left(\\sum_{i=1}^C \\mathbf{x}_i \\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "Where the vector $\\mathbf{x}$ in \\ref{eq:h} is our sentence vector which is simply the average of all the word vectors that make up the sentence. You may have noticed that in `word2vec` **the hidden layer has no activation function**. \n",
    "\n",
    "Now, the next step is to compute the **inputs** to each node on the outer layer, which we will define as $u$. For the $j^{th}$ output node we have\n",
    "$$\n",
    "\\begin{equation} \n",
    "u_j = {\\mathbf{v}^\\prime_{\\mathbf{W}_j}}^T \\cdot \\mathbf{h} \n",
    "\\end{equation}\n",
    "$$\n",
    "Where ${\\mathbf{v}^\\prime_{\\mathbf{W}_j}}$ is the $j^{th}$ column of the output matrix $\\mathbf{W}^\\prime$. Finally, we then compute the elements of the output (one-hot encoded) vector $\\mathbf{y}$ as a Boltzmann distribution which can be thought of as the 'activation' function of this network.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\tag{2}\n",
    "\\label{eq:yi}\n",
    "y_j  = p(w_{y_j} | w_1, ..., w_C) = \\frac{\\exp(u_{y_j})}{\\sum_{i=1}^V \\exp(u_i) }\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where the subscript $y_j$ means \"the index of the $j^{th}$ output of $\\mathbf{y}$. Now we have a definition of our word embeddings, namely:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y(\\mbox{word}) = \\mbox{argmax} \\frac{\\exp(u_{y_j})}{\\sum_{i=1}^V \\exp(u_i) } = u_{y_j = index(\\mbox{word})} = {\\mathbf{v}^\\prime_{\\mathbf{W}_{j = index(\\mbox{word})}}}^T \\cdot \\mathbf{h} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where we have switched to a more obvious notation, but less clear for derivations.  \n",
    "\n",
    "As once our neural network is fully trained, we will have $V$ _word embeddings_ of size $N$, the size of our hidden layer. This is because we will now have a mapping from our one-hot encoded output vectors of a specific word to the specific _word embedding_ $u_j$ from equation \\ref{eq:yi}. Indeed, the index $j$ where $\\mathbf{y}$ is maximal will represent the column of the matrix $\\mathbf{u}$ which represents the embedding of the output word $\\mathbf{y}$\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary> Click here to learn about `word2vec back propagation </summary>\n",
    "<br>\n",
    "<h2> Finding the weights of the neural network  </h2>\n",
    "\n",
    "\n",
    "\n",
    "### Learning $\\mathbf{W}$ and $\\mathbf{W}^\\prime$\n",
    "As the training weights define our word embeddings, it is important to discuss how the randomly initialized weights get updated to effectively represent our word embeddings. \n",
    "\n",
    "### Updating $\\mathbf{W^\\prime}$\n",
    "As with any neural network, the first step is to define our loss function, in this case we want to maximize the conditional probability (log likelihood) of our output word vector $\\mathbf{y}$ given our input context sentence vector $\\mathbf{x}$. So, for our first matrix $\\mathbf{W^\\prime}$\n",
    "$$\n",
    "\\begin{align}\n",
    "E & = - \\ln p(\\mathbf{y_*} | \\mathbf{x}) \\\\\n",
    "  & = -\\ln \\left(  \\frac{\\exp(u_j)}{\\sum_{i=1}^V \\exp(u_i) }\\right) \\\\\n",
    "  & = -u_* + \\ln \\sum_{i=1}^V \\exp{u_i} \\\\\n",
    "  & = -{\\mathbf{v}_{\\mathbf{w_O}}}^T \\cdot \\mathbf{h} + \\ln \\sum_{i=1}^V \\exp({\\mathbf{v}_{\\mathbf{w}_i}}^T \\cdot \\mathbf{h})\n",
    "\\end{align}  \n",
    "$$\n",
    "Where the $*$ subscript on $u_*$ is to indicate the index of the actual output word (our target). Now, let's write out our derivatives explicitly for back propagation. First up,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial u_j} & = \\frac{\\partial}{\\partial u_j} \\left( -u_* + \\ln \\sum_{i=1}^V \\exp{u_i} \\right) \\\\\n",
    "& = -\\delta_{j,*} + \\frac{\\partial}{\\partial u_j}\\ln \\sum_{i=1}^V \\exp{u_i} \\\\\n",
    "& = -\\delta_{j,*} + \\frac{1}{\\sum_{i=1}^V \\exp{u_i} }  \\frac{\\partial}{\\partial u_j}\\sum_{i=1}^V \\exp{u_i} \\\\\n",
    "& = -\\delta_{j,*} + \\frac{\\exp(u_j)}{\\sum_{i=1}^V \\exp(u_i) } \\\\\n",
    "& = y_i - \\delta_{j,*}\n",
    "\\end{align}\n",
    "$$\n",
    "Where $\\delta_{j,*}$ is the Kronecker delta defined as \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_{j, *} =\n",
    "\\begin{cases}\n",
    "        1, &         \\text{if } j=*,\\\\\n",
    "        0, &         \\text{if } *\\neq *.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now that we have the derivative of our loss function with respect to the input to the $j^{th}$ node of our output layer $u_j$, we can also calculate the partial derivative with respect to the output weight $w^\\prime_{i,j}$ via the chain rule\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w^\\prime_{i,j}} & =\\frac{\\partial E}{\\partial u_j} \\frac{\\partial u_j}{\\partial w^\\prime_{i,j}} \\\\\n",
    "& = (y_j - \\delta_{j,*} ) \\frac{\\partial u_j}{\\partial w^\\prime_{i,j}} \\\\ \n",
    "& = (y_j - \\delta_{j,*} ) \\cdot \\frac{\\partial}{\\partial w^\\prime_{i,j}}\\left( {\\mathbf{v}^\\prime_{\\mathbf{W}_j}}^T \\cdot \\mathbf{h} \\right) \\\\\n",
    "& = (y_j - \\delta_{j,*} ) \\cdot \\frac{\\partial}{\\partial w^\\prime_{i,j}}\\left( \\sum_k w_{k,j} h_k \\right) \\\\\n",
    "& = (y_j - \\delta_{j,*} ) \\cdot h_i\n",
    "\\end{align}\n",
    "\n",
    "Fantastic! Now we can define an update equation using gradient decent with learning rate $0 < \\eta < 1 $ as\n",
    "\n",
    "\\begin{equation}\n",
    "w_{i,j}^{\\prime (new)} = w_{i,j}^{\\prime (old)} - \\eta (y_j - \\delta_{j,*} ) \\cdot h_i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "or, in vector notation \n",
    "\\begin{equation}\n",
    "\\mathbf{v}_{\\mathbf{w}_j}^{\\prime(new)} =\\mathbf{v}_{\\mathbf{w}_j}^{\\prime(old)} - \\eta (y_j - \\delta_{j,*} ) \\cdot \\mathbf{h}\n",
    "\\end{equation}\n",
    "\n",
    "Where recall that $\\mathbf{v}_{\\mathbf{w}_j}^{\\prime}$ is the $j^{th}$ vector of our weight matrix $\\mathbf{W}^\\prime$\n",
    "\n",
    "### Updating $\\mathbf{W}$\n",
    "\n",
    "We also have a similar equation for the input weights. Our first derivative will be taken with respect to an arbitrary hidden node $h_i$\n",
    "$$\n",
    "\\begin{align} \n",
    "\\frac{\\partial E}{\\partial h_i} &= \\sum_{j=1}^V \\frac{\\partial E}{\\partial u_j} \\cdot \\frac{\\partial u_j}{\\partial h_i} \\\\\n",
    "& = \\sum_{j=1}^V (y_j - \\delta_{j, *} )\\cdot \\frac{\\partial}{\\partial h_i} \\left( \\sum_j w^\\prime_{i,j} h_j \\right) \\\\\n",
    "& = \\sum_{j=1}^V (y_j - \\delta_{j, *}) \\cdot w^\\prime_{i,j} \n",
    "\\end{align}\n",
    "$$\n",
    "Where the sum is a result of the hidden layer being fully connected. The next derivative is the derivative of our loss function with respect to an arbitrary weight $w_{k,i}$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w_{k,i} } & = \\frac{\\partial E}{\\partial h_i} \\cdot \\frac{\\partial h_i}{\\partial w_{k,i}} \\\\\n",
    "& = \\sum_{j=1}^V (y_j - \\delta_{j, *} )\\cdot w^\\prime_{i,j} \\cdot \\frac{\\partial}{\\partial w_{k,i}}\\left[ \\frac{1}{C} \\mathbf{W} \\cdot \\left(\\sum_{l=1}^C \\mathbf{x}_l \\right) \\right]\\\\ \n",
    "& = \\frac{1}{C} \\left( \\mathbf{x} \\cdot \\sum_{j=1}^V (y_j - \\delta_{j, *} )\\right) \\cdot \\frac{1}{C} \\cdot x_k \\\\\n",
    "& \\Rightarrow \\frac{1}{C} (\\mathbf{x} \\cdot EH) \n",
    "\\end{align}\n",
    "$$\n",
    "Where \n",
    "$$\n",
    "\\begin{equation}\n",
    "EH = \\frac{1}{C} \\left( \\sum_{j=1}^V (y_j - \\delta_{j, *} )\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "and $\\mathbf{x}$ becomes a vector after summation over $k$. In this case, the final gradient update equation for the input weight is \n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\mathbf{v}_{\\mathbf{w_{I,c}}}^{\\prime}}^{(new)} = {\\mathbf{v}_{\\mathbf{w_{I,c}}}^{\\prime}}^{(old)} - \\frac{\\eta}{C} EH\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop!\n",
    "\n",
    "Before you continue, please run the cell below. Once it completes, please click `Kernel -> Restart Kernel` from the top menu before running the cells through the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --user\n",
    "!pip install gensim --upgrade --user "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Tweets and Word Embeddings\n",
    "\n",
    "In the cell below we're downloading our cleaned tweets once again from the Rapid Access Cloud and putting them into a data frame. We also define our `model`, which is our pre-determined word embeddings that we can use to understand word similarity mathematically. \n",
    "\n",
    "**Note** This first cell may take a few moments to run as it's downloading ~ a 300 mb file to your hub account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at_date</th>\n",
       "      <th>hashtags_string</th>\n",
       "      <th>user_string</th>\n",
       "      <th>user_location</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>extended_tweet_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-18 10:23:54-07:00</td>\n",
       "      <td>xmas nicelist</td>\n",
       "      <td>CallawayGolfEU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jeff ullock</td>\n",
       "      <td>JeffUllock</td>\n",
       "      <td>@CallawayGolfEU The crows nest hands down! #xm...</td>\n",
       "      <td>The crows nest hands down!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-18 10:24:19-07:00</td>\n",
       "      <td>highschooldesign</td>\n",
       "      <td></td>\n",
       "      <td>Kitscoty, Alberta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jackie Bouck</td>\n",
       "      <td>JackieBouck</td>\n",
       "      <td>Does anyone know of any high schools that have...</td>\n",
       "      <td>Does anyone know of any high schools that have...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at_date   hashtags_string     user_string  \\\n",
       "0 2018-12-18 10:23:54-07:00     xmas nicelist  CallawayGolfEU   \n",
       "1 2018-12-18 10:24:19-07:00  highschooldesign                   \n",
       "\n",
       "       user_location  longitude  latitude          name  screen_name  \\\n",
       "0                NaN        NaN       NaN   jeff ullock   JeffUllock   \n",
       "1  Kitscoty, Alberta        NaN       NaN  Jackie Bouck  JackieBouck   \n",
       "\n",
       "                                      extended_tweet  \\\n",
       "0  @CallawayGolfEU The crows nest hands down! #xm...   \n",
       "1  Does anyone know of any high schools that have...   \n",
       "\n",
       "                              extended_tweet_cleaned  \n",
       "0                     The crows nest hands down!      \n",
       "1  Does anyone know of any high schools that have...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import urllib.request\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import \n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np \n",
    "from numpy.linalg import norm\n",
    "\n",
    "# download word vectors\n",
    "model_url = \"https://swift-yeg.cloud.cybera.ca:8080/v1/AUTH_233e84cd313945c992b4b585f7b9125d/geeky-summit/GoogleNews-vectors-negative300-SLIM.bin\"\n",
    "model_file = 'GoogleNews-vectors-negative300-SLIM.bin'\n",
    "urllib.request.urlretrieve(model_url, model_file)\n",
    "\n",
    "# Load pretrained model \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_file, binary=True)\n",
    "\n",
    "# download tweets\n",
    "target_url=\"https://swift-yeg.cloud.cybera.ca:8080/v1/AUTH_233e84cd313945c992b4b585f7b9125d/geeky-summit/alberta_tweets_cleaned.csv\"\n",
    "file_name=\"alberta_tweets_cleaned.csv\"\n",
    "urllib.request.urlretrieve(target_url, file_name)\n",
    "\n",
    "df = pd.read_csv(file_name,parse_dates=['created_at_date']) \n",
    "# Convert to mountain standard time \n",
    "df.created_at_date= df.created_at_date.dt.tz_localize('UTC').dt.tz_convert('MST')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Test Drive\n",
    "\n",
    "Before we dive into the analysis, let's take a look at these \"word embeddings\" that we've now discussed at length. In this case, let's look at the embedding for the word \"chocolate\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the 'len' function and its enclosing parenthesis to look at the vector \n",
    "# directly if you're interested \n",
    "len(model.wv['chocolate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is that the word embedding is a 300 dimensional vector. This means two things:\n",
    "1. The neural network these embeddings were taken from had a hidden layer of size 300\n",
    "2. All words that this network was trained on (in this case approximately 300000) have been mapped down do 300 dimensional vectors, rather than their original sparse 300000 dimensional vectors. What a space savings!\n",
    "\n",
    "Now that we have smaller vectors what can we do with them? As these are now \"dense\" vectors, we can start to do more exciting vector things with them. For example, we can calculate how \"close\" these vectors are to each other.  The idea being that the closer two word embeddings are, the more similar in meaning the words they represent are. In this case, vector similarity is calculated using the cosine distance between two vectors $\\mathbf{A}$ and $\\mathbf{B}$ is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{||\\mathbf{A}|| ||\\mathbf{B}||} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\sqrt{\\sum_i A_i^2} \\sqrt{\\sum_i B_i^2} } = \\left[-1, 1\\right]\n",
    "\\end{equation}\n",
    "\n",
    "where this is simply the dot product of the normalized vectors. A similarity, often called a cosine distance, can take values between -1 and 1. Values of -1 imply that the vectors are anti-parallel, and in the context of word embeddings these words can be considered to be \"opposite\". A similarity of 0 implies the vectors are orthogonal, meaning that these words are not similar. Finally, a similarity score of 1 implies they're the same vector, or the same word. **The closer to one the similarity between two word embeddings are, the more similar the words are**.\n",
    "\n",
    "Let's take that for a spin, first, lets see the most similar words to \"chocolate\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chocolates', 0.7569215893745422),\n",
       " ('Chocolate', 0.7080809473991394),\n",
       " ('caramel', 0.673201322555542),\n",
       " ('caramels', 0.6604536175727844),\n",
       " ('candy', 0.6375688910484314),\n",
       " ('candies', 0.6303936839103699),\n",
       " ('nougat', 0.6247628927230835),\n",
       " ('macaroons', 0.6229639649391174),\n",
       " ('Chocolates', 0.6201929450035095),\n",
       " ('choco', 0.612962543964386)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we're getting the 10 most similar words to the word \"chocolate\"\n",
    "model.most_similar(positive=['chocolate'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output above, what see is the word as the first output, and the cosine similarity as the number reported. Feel free to try other words in the above example.\n",
    "\n",
    "We can also add and subtract these word embeddings to discover how words may be related in more complex ways. For example we can return results like \n",
    "\n",
    "> Man is to King as woman is to ____ \n",
    "\n",
    "Where we would fill in the word \"Queen\" by hand. In terms of vectors, this is done as \n",
    "\n",
    "$$ \\mathbf{V}(queen) = \\mathbf{V}(king) - \\mathbf{V}(man) + \\mathbf{V}(woman) $$\n",
    "\n",
    "\n",
    "Which can be done using our model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189675331115723),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('prince', 0.5377322435379028),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454),\n",
       " ('throne', 0.5005807876586914),\n",
       " ('royal', 0.4938204884529114)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive = ['king','woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which, is exciting in the sense that now **anything we can do to a vector, we can now do with language**.\n",
    "\n",
    "## Preparing Tweets\n",
    "\n",
    "Our goal now is to create \"tweet\" vectors by adding up all the word vectors in our tweets. Mathematically we're after something of the form \n",
    "\\begin{equation}\n",
    "\\mathbf{V}_{tweet} = \\frac{1}{N}\\sum_{word}^N \\mathbf{V}_{word}\n",
    "\\end{equation}\n",
    "where $N$ is the number of words in our tweet. \n",
    "\n",
    "But first - we have to do what we always do: clean our tweets even more. The first step, as we've seen before is to remove stop words and turn our tweets into tokens. In this case, the reason we're removing stop words is because they don't add anything \"meaningful\" to our sentences, and if we keep them in we'll accidentally make tweets look more similar to each other than they are because they're cluttered with filler language! This is done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alextennant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.append('amp')\n",
    "print(stop_words[1:10])\n",
    "stop_words = set(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tweets to analyze after cleaning:  23380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at_date</th>\n",
       "      <th>hashtags_string</th>\n",
       "      <th>user_string</th>\n",
       "      <th>user_location</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>extended_tweet_cleaned</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-18 10:24:19-07:00</td>\n",
       "      <td>highschooldesign</td>\n",
       "      <td></td>\n",
       "      <td>Kitscoty, Alberta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jackie Bouck</td>\n",
       "      <td>JackieBouck</td>\n",
       "      <td>Does anyone know of any high schools that have...</td>\n",
       "      <td>Does anyone know of any high schools that have...</td>\n",
       "      <td>[does, anyone, know, any, high, schools, that,...</td>\n",
       "      <td>[anyone, know, high, schools, pods, grade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-18 10:24:30-07:00</td>\n",
       "      <td></td>\n",
       "      <td>miss9afi</td>\n",
       "      <td>Alberta, Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Faiz Ali Qureshi 🇨🇦</td>\n",
       "      <td>FKamranAB</td>\n",
       "      <td>@miss9afi That's blasphemy law you can say  it...</td>\n",
       "      <td>That's blasphemy law you can say  it's like ...</td>\n",
       "      <td>[that, blasphemy, law, you, can, say, like, ho...</td>\n",
       "      <td>[blasphemy, law, say, like, holocaust, law, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-18 10:24:44-07:00</td>\n",
       "      <td></td>\n",
       "      <td>ABLiberal</td>\n",
       "      <td>Calgary, Alberta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Irwin Singh Brar</td>\n",
       "      <td>roti888</td>\n",
       "      <td>Albertans voted for @ABLiberal in 1993-2012 th...</td>\n",
       "      <td>Albertans voted for   in 1993-2012 they were o...</td>\n",
       "      <td>[albertans, voted, for, they, were, official, ...</td>\n",
       "      <td>[albertans, voted, official, opposition, nearl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at_date   hashtags_string user_string      user_location  \\\n",
       "1 2018-12-18 10:24:19-07:00  highschooldesign              Kitscoty, Alberta   \n",
       "2 2018-12-18 10:24:30-07:00                      miss9afi    Alberta, Canada   \n",
       "3 2018-12-18 10:24:44-07:00                     ABLiberal   Calgary, Alberta   \n",
       "\n",
       "   longitude  latitude                 name  screen_name  \\\n",
       "1        NaN       NaN         Jackie Bouck  JackieBouck   \n",
       "2        NaN       NaN  Faiz Ali Qureshi 🇨🇦    FKamranAB   \n",
       "3        NaN       NaN     Irwin Singh Brar      roti888   \n",
       "\n",
       "                                      extended_tweet  \\\n",
       "1  Does anyone know of any high schools that have...   \n",
       "2  @miss9afi That's blasphemy law you can say  it...   \n",
       "3  Albertans voted for @ABLiberal in 1993-2012 th...   \n",
       "\n",
       "                              extended_tweet_cleaned  \\\n",
       "1  Does anyone know of any high schools that have...   \n",
       "2    That's blasphemy law you can say  it's like ...   \n",
       "3  Albertans voted for   in 1993-2012 they were o...   \n",
       "\n",
       "                                              tokens  \\\n",
       "1  [does, anyone, know, any, high, schools, that,...   \n",
       "2  [that, blasphemy, law, you, can, say, like, ho...   \n",
       "3  [albertans, voted, for, they, were, official, ...   \n",
       "\n",
       "                                               lemma  \n",
       "1         [anyone, know, high, schools, pods, grade]  \n",
       "2  [blasphemy, law, say, like, holocaust, law, we...  \n",
       "3  [albertans, voted, official, opposition, nearl...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = df.copy()\n",
    "\n",
    "# tokenization and punctuation removal\n",
    "def sent_to_words(row):\n",
    "    sentence = row.extended_tweet_cleaned\n",
    "    return(gensim.utils.simple_preprocess(sentence,min_len=3))  \n",
    "\n",
    "# Create a new \"tokens\" colmn with our sent_to_words function\n",
    "subset['tokens'] = subset.apply(sent_to_words, axis=1)\n",
    "\n",
    "# Wrapper function to clean all the text at once \n",
    "def tokens_to_lda(row):\n",
    "    data_token = row.tokens\n",
    "    tokens = [token for token in data_token if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "## Now we create a 'lemma' column for our super cleaned data!\n",
    "subset['lemma'] = subset.apply(tokens_to_lda, axis=1)\n",
    "\n",
    "# # We are also removing all data where the tweet is less than four words long. \n",
    "subset = subset[subset['lemma'].apply(lambda x: len(x) > 3)]\n",
    "\n",
    "print(\"Total Tweets to analyze after cleaning: \", len(subset))\n",
    "subset.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding \"Tweet Vectors\"\n",
    "\n",
    "To find tweet vectors, we simply take the average of all the word vectors in our cleaned and processed tweets. This is done below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here we are making a list of all the words we have vectors for \n",
    "vocab = model.vocab.keys()\n",
    "\n",
    "def tweet_vectorizer(tweet):\n",
    "    vector = [0 for i in range(300)]\n",
    "    count = 0\n",
    "    # Here we build our tweet vectors \n",
    "    for w in tweet:\n",
    "        if w in vocab:\n",
    "            count += 1\n",
    "            vector = vector + model[w] \n",
    "    if len(tweet) != 0:\n",
    "        # Divide by the norm here so we can be lazy with a matrix multiplication later. \n",
    "        vector = np.array(vector) / count\n",
    "        vector = vector/norm(vector)\n",
    "        return list(vector)\n",
    "    else:\n",
    "        # If no words in the tweet are in the model, we set it to zero\n",
    "        # to ensure it will be opposite to all other word embeddings. \n",
    "        return [0 for i in range(300)]\n",
    "\n",
    "# Actually call the function \n",
    "subset['vect'] = subset['lemma'].apply(tweet_vectorizer)\n",
    "subset = subset.reset_index(drop=True)\n",
    "\n",
    "# Here we truncate our set\n",
    "subset = subset[0:20000]\n",
    "subset.tail(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Similar Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now created our own \"tweet\" vectors using our pre-trained model, let's take a look and see how well it worked! First, try out the code below. You will have to write a little code first however! But, note in the cell below \n",
    "\n",
    "\\begin{equation}\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "\\begin{bmatrix}\n",
    "& & V_{\\mbox{tweet } 1} & & & \n",
    "\\end{bmatrix} \\\\\n",
    " \\begin{matrix}\n",
    " & & \\dots & & & \n",
    "\\end{matrix} \\\\\n",
    " \\begin{matrix}\n",
    " & & \\dots & & & \n",
    "\\end{matrix} \\\\\n",
    "\\begin{bmatrix}\n",
    "& & V_{\\mbox{tweet } N} & & & \n",
    "\\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Is the matrix of all our tweet vectors, where each row is an individual tweet vector. Keep in mind we've also **already normalized all our tweet vectors**. So you don't have to worry about normalizing things again when you're calculating your similarity scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the matrix A\n",
    "A = np.array(subset['vect'].tolist())\n",
    "print(A.shape)\n",
    "\n",
    "# index of the tweet we're interested in, feel free to change this. \n",
    "# Note, that it's currently set to the last tweet, so your choices must be > 19999\n",
    "interest = 19999\n",
    "# this is the tweet vector at the index we've decided to use \n",
    "target = np.array(subset['vect'].iloc[interest])\n",
    "\n",
    "# I will remove this to be an exersize YOUR CODE HERE\n",
    "myvals = A @ target \n",
    "\n",
    "max_index = myvals.argsort()[-5:][::-1]\n",
    "# if you're interested in the _least_ similar tweets, try changing thngs with this\n",
    "# min_index = np.argpartition(myvals, 5)[:5]\n",
    "\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"Note: The first most similar tweet will be the tweet itself\")\n",
    "print(\"Tweet we're looking for similarities to:\\n\\n\",subset['extended_tweet_cleaned'].iloc[interest])\n",
    "print(\"=\"*90)\n",
    "print()\n",
    "for index in max_index:\n",
    "    print(subset['extended_tweet_cleaned'].iloc[index],\"||\" ,\"index =\", index,  \"similarity score=\", myvals[index])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to search for different tweets which are similar. There will be some cases where it works quite well, and some cases where... well it probably didn't do the best job. Unfortunately, that's just the nature of twitter - this model can't account for things like spelling mistakes, emojis, and the typical non-sense that one might expect from within 20000 tweets. But, there will certainly be cases where you may be surprised at how well this method works from nothing but a simple matrix-vector dot product!\n",
    "\n",
    "# K-means clustering as a topic model\n",
    "\n",
    "As we mentioned earlier, as our tweets are now vectors, we are able to do \"vector things\" with them. One such \"vector thing\" is the ability to cluster vectors together based on similarity. This means that we can group similar vectors together and create multiple groups of similar vectors, in our case, tweets that contain similar language used. Each of these groups may represent a topic, and we can use these topics to better understand what people are discussing in these tweets - and if we can use that to better understand something of interest to us. \n",
    "\n",
    "## K-means\n",
    "\n",
    "K-means is an algorithm that will group similar vectors (data) around $k$ centers, where the number of centers $k$ is determined in advance. Essentially, K-means will sort our vectors based on similarity, and group similar tweets together for us. K-means clustering is deceptively simple and can be described as the following iterative steps\n",
    "\n",
    "1. Initialize by defining $k$ centroids _randomly_. With our tweets, these centroids will be 300 dimensional vectors.\n",
    "2. Repeat the following until changes between iterations are minimal\n",
    "      - Assign each data point (our tweet vectors) to the centroid to which it is closest to. \n",
    "      - Recalculate the positions of the centers based on the mean of the data points that were assigned to it.\n",
    "    \n",
    "If you're interested in learning more about K-means clustering, expand the drop down below, however an intimate understanding of the algorithm is not required to complete the rest of this tutorial.\n",
    "\n",
    "---\n",
    "<details>\n",
    "<summary> Click here to learn more about the k-means algorithm </summary>\n",
    "<br>\n",
    "\n",
    "## The K-means clustering algorithm\n",
    "In this case, we define a set of centers \n",
    "\\begin{equation}\n",
    "means =\\left\\{\\mu_i\\right\\} |_{i=0}^k \n",
    "\\end{equation} \n",
    "where $k>1$, the number of centers you'd like to look for (and in our case, the number of topics). \n",
    "\n",
    "Once we've decided on the number of topics, the objective of k-means is to find \n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{3}\n",
    "\\label{eq:kmean}\n",
    "f(\\mathbf{\\mu};\\mathbf{x}) =\\min \\sum_{j \\in k} \\sum_{i \\in S_j} ||x_i - \\mu_j||^2\n",
    "\\end{equation}\n",
    "\n",
    "where the sets $S_j$ are the sets of points which are closest to center $\\mu_j$. In order to find the appropriate means $\\mu_j$ an iterative procedure is used. Specifically, we have the following algorithm. \n",
    "\n",
    "---\n",
    "**initialize** $\\mu_1, ..., \\mu_k \\rightarrow$ our randomly chosen centers <br>\n",
    "\n",
    "**While** objective function \\ref{eq:kmean} improves **do**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **Assign** each $\\mathbf{x}_i$ to a set $S_1,.., S_k$ based on euclidean distance from each center $\\mu_j$: <br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp; **for** $i$ **in range(N)** $\\leftarrow$ $N =$Number of data points <br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $j =  \\mbox{argmin} ||x_i - \\mu_k||^2$ <br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; add $\\mathbf{x}_j$ to $S_j$ <br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp; **end for** <br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp; **for** $j = 1,...,k$: <br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\mu_j$ =$ \\frac{1}{\\mbox{len}(S_j)}\\sum_{i \\in S_j} x_i$ $\\leftarrow$ calculate new centers<br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;**end for** <br>\n",
    "**end while**\n",
    " \n",
    "\n",
    "\n",
    "Once that algorithm has converged, we will have a set of $k$ optimize means, and each data point will be assigned to the mean that is closest to it. \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing $k$, the number of clusters\n",
    "\n",
    "As the number of clusters $k$ is required for our algorithm, how do we decide on how many clusters we should have? Well, this is a case where Data Science becomes more art than science. In this case, we will opt for an \"elbow plot\", or a plot where we calculate a metric for how well our topic model performs, and the \"ideal\" number of topics is where further iterations of the model stop improving. This is done in the cell block below. However, it can take a moment to run, so it is currently commented out. If you'd like to generate your own figure, certainly feel free but note it may take 5-10 minutes to calculate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# from sklearn.cluster import MiniBatchKMeans\n",
    "# inert = []\n",
    "# for i in range(2,200):\n",
    "#     clear_output()\n",
    "#     print(i)\n",
    "#     # here we're simply calculating more clusters each iteration. \n",
    "#     intertia_test = MiniBatchKMeans(init='k-means++', n_clusters=i+1, random_state=5, init_size=3*i)\n",
    "#     # This is where we fit our data to our k-means topics \n",
    "#     intertia_test.fit(A)\n",
    "#     # WE also save what's known as the model inertia - which is related to how far away each data point \n",
    "#     # is from a given topic. A smaller inertia indicates a \"better\" model. \n",
    "#     inert.append(intertia_test.inertia_)\n",
    "\n",
    "# plt.figure(figsize=(12,8))   \n",
    "# plt.xlabel(\"Number of clusters\", size = 16)\n",
    "# plt.ylabel(\"Momentum (smaller is better)\", size=16)\n",
    "# plt.plot(inert)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code cell above, you should get a plot which looks like the one below \n",
    "\n",
    "> ![elbow plot](images/inertia.png)\n",
    "> Figure: This figure is the \"elbow\" plot which will aide us in choosing the best number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above, we see that the \"momentum\" of our plot, or the objective function, gets smaller and smaller as we add more and more topics. In this case, the \"elbow\" is at approximately 77 topics, and for many applications, that may be the number of topics you wish to choose. However, for our analysis we're going to choose the full 200 as we have reason to believe that out of 20000 tweets, we probably have more than 77 topics at a given time. This is done below, however, feel free to experiment with the `n_clusters` parameter in the cell below and experiment with different topic models. Each model typically only takes a second or two to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "# Define the k-means model\n",
    "kmeans_model = MiniBatchKMeans(init='k-means++', n_clusters=200, random_state=5)\n",
    "# Pass it the matrix A, or all out tweets!\n",
    "kmeans_model.fit(A)\n",
    "cluster_labels  = kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing The Topic Model \n",
    "\n",
    "Now that we've created a topic model, it might be nice to test how well it did! In this case, let's see how k-means clustered tweets from an obvious source. We're lucky in that there is a twitter account `@311Calgary` which tweets out things like road reports, dead animal pick ups, traffic light fixes and so forth. The idea here however is that many of these tweets are very formulaic, and if our topic model performed admirably, these tweets should be clustered well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets311 =subset[subset.screen_name == '311calgary'].index.tolist()\n",
    "places=[]\n",
    "for labels in tweets311:\n",
    "    # Here cluster_labels[labels] returns the topic that each tweet has been assigned to \n",
    "    places.append(cluster_labels[labels])\n",
    "# set removes duplicates\n",
    "print(set(places))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the numbers above are all the topics where the tweets from `@311calgary` got sorted. Explore each topic and see how well you think our model did. Some topcis (such as 48, the default below) seem to be very well sorted. However, as you explore see if you can find some exceptions - but also read further to see if you can understand _why_ something may have been grouped \"incorrectly\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change cluster_labels == NUM to view different topics \n",
    "itemindex = np.where(cluster_labels== 48)[0]\n",
    "itemindex\n",
    "c=0\n",
    "for i in itemindex:\n",
    "    c +=1 \n",
    "    print(subset['extended_tweet_cleaned'].iloc[i])\n",
    "    print()\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you view the topics that our 311 Calgary tweets got placed into - you'll notice that different kinds of reports got placed in their own topic automatically such as street light reports (topic 48) and dead animal pickup (topic 50). This was all done automatically - we did nothing to ensure that this happened. This is simply a result of the similar language used in each of the `311Calgary` tweets being very similar, which makes them easy to cluster. \n",
    "\n",
    "# Finding Topics You Care About\n",
    "\n",
    "From the above, you've probably noticed that we just have a bunch of numbers representing topics - we still have no idea what those topics actually are! Unfortunately that's not something we can get away from, in any form of clustering, it is still up to us, a human, to decide what the topic is actually about. However, we can \"cheat\" a little and add data to our data set which we feel represents a topic we're interested in, and use that to find a topic we care about.\n",
    "\n",
    "\n",
    "What we're going to do next is create some \"fake\" data and see if we can find some topics about food and sports. How we're going to do this is insert our own tweets into the data, and re-run the topic model with these new tweets. Then, we'll simply see where our fake tweets ended up!\n",
    "\n",
    "Before we get into it, remember that we're essentially adding custom word vectors - and the word vectors don't care about things like word order, grammar, or filler words. We essentially just need to do a **key word search**. Let's take a look at how that's done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new data frame to not mess anything else up \n",
    "subset_with_discovery = df.copy()\n",
    "\n",
    "# here are the sentences we're adding \n",
    "sentences = [\n",
    "    \"I am hungry what should I eat in town? looking for recommendations\",\n",
    "    \"I could really go for brunch right now\",\n",
    "    \"I sure could go for a meal. what's the best pizza in town?\",\n",
    "    \"Come down to for the best food in town!\",\n",
    "    \"breakfast is the most important meal of the day\",\n",
    "    \"what should I eat for dinner? i've got a craving for junk food\",\n",
    "    \"i am lazy today, what should i order for dinner to feed myself\",\n",
    "    \"thinking about ordering in some food for dinner\",\n",
    "    \"hockey garbage bad goal\",\n",
    "    \"sports! games! winning! boy do I enjoy these atheletes\",\n",
    "]\n",
    "\n",
    "# Insert a fake user so they're easy to find like the 311calgary tweets \n",
    "user = 'fake_tweets_for_analysis'\n",
    "\n",
    "# Need to fill in the other data to the table, here we're just entering nothing \n",
    "add = {\n",
    "    'created_at_date':[np.nan for i in range(len(sentences))],\n",
    "    'hashtags_string':[np.nan for i in range(len(sentences))],\n",
    "    'user_string':[np.nan for i in range(len(sentences))],\n",
    "    'user_location':[np.nan for i in range(len(sentences))],\n",
    "    'longitude':[np.nan for i in range(len(sentences))],\n",
    "    'latitude':[np.nan for i in range(len(sentences))],\n",
    "    'name':[np.nan for i in range(len(sentences))],\n",
    "    'screen_name':[user for i in range(len(sentences))],\n",
    "    'extended_tweet':[np.nan for i in range(len(sentences))],\n",
    "    'extended_tweet_cleaned':[sentences[i] for i in range(len(sentences))]\n",
    "}\n",
    "    \n",
    "add = pd.DataFrame.from_dict(add)\n",
    "\n",
    "# We also need to treat this data just like the original data! Nothing new here \n",
    "add['tokens'] = add.apply(sent_to_words, axis=1)\n",
    "add['lemma'] = add.apply(tokens_to_lda, axis=1)\n",
    "\n",
    "add['vect'] = add['lemma'].apply(tweet_vectorizer)\n",
    "new_subset = pd.concat([subset, add]).reset_index(drop=True)\n",
    "\n",
    "print(\"Total Tweets to analyze after cleaning: \", len(new_subset))\n",
    "# This is to just look at our inserted fake tweets \n",
    "new_subset.tail(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've created our fake data and inserted it into our set, let's make a new kmeans model and see where those ended up! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = np.array(new_subset['vect'].tolist())\n",
    "print(A.shape)\n",
    "kmeans_model = MiniBatchKMeans(init='k-means++', n_clusters=200, random_state=5)\n",
    "kmeans_model.fit(A)\n",
    "cluster_labels  = kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same code as our 311Calgary tweets, lets' see where our nefarious fake tweets have landed in the topic model, and judge for ourselves how well they worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places=[]\n",
    "infultrators = new_subset[new_subset.screen_name == user].index.tolist()\n",
    "for labels in infultrators:\n",
    "    places.append(cluster_labels[labels])\n",
    "print(\"Our fake tweets of 'known' topic gave ended up in the following topics:\\n\", places)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at where our \"infiltration\" tweets ended up \n",
    "\n",
    "Let's start with topic `8`, which is our sports topic and see how well it did. This is done below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemindex = np.where(cluster_labels== 8)[0]\n",
    "itemindex\n",
    "c=0\n",
    "for i in itemindex:\n",
    "    c +=1 \n",
    "    print(new_subset['extended_tweet'].iloc[i])\n",
    "    print()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably it did pretty well, certainly not all the tweets are about sports, but considering we were able to sort these into 200 topics in a few seconds, it did pretty good!\n",
    "\n",
    "### For you\n",
    "Try adding your own key word searches into the model and re running it. Do the topics your fake tweets end up in make sense? What do you think you could do to make things a little better? Also try things like a different number of topics - how do your results chance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "From here you've got a great baseline to start things like market research. If you find a large group of tweets relevant to either your product or business, you can use those to understand how people are discussing those issues publicly. Either from a sentiment analysis to understand how people feel, as market research to understand what people seem to need, or to discover which users to target for advertising purposes. Of course, this is a very minimal and may not be perfect for answering those questions explicitly. However, understanding this model can act as a solid basis to iterate to more complex models which may see greater success in understanding your business problems via twitter text analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
