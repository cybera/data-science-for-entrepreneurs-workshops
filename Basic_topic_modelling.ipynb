{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: basic topic modelling for Twitter data\n",
    "In this notebook, we are going to use our cleaned dataset that we have created in the second notebook.\n",
    "\n",
    "We are going to do some basic Natural Language Processing (NLP) - we will create a topic model for the `extended_tweet_cleaned` column.\n",
    "\n",
    "## Goal: \n",
    "Learn what topic modelling is (specifically LDA), apply topic modelling algoithms on different subsets of the input data, learn how to visualize topics.\n",
    "\n",
    "## Introduction to LDA\n",
    "[LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)(Latent Dirichlet Allocation) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.(Wikipedia)\n",
    "\n",
    "\n",
    "LDA divides documents (tweets) into topics(clusters). \n",
    "It assumes that:\n",
    " - every topic (e.g. sport) has some representative words (e.g soccer, play, ball. etc.), \n",
    " - every document(tweet) has some representative topics(e.g sport, summer, etc.).\n",
    "\n",
    "Topics can interfere: the same word can belong to multiple topics.  \n",
    "Documents (tweets) can have multiple topics.\n",
    "\n",
    "### Topic modelling process:\n",
    " - Subset the data\n",
    " - Prepare data for topic modelling(tokenize, lemmatize)\n",
    " - Apply topic modelling\n",
    " - Vizualize and explore\n",
    " - Repeat if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets download the cleaned tiwitter data from object storage and display first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load python libraries first. Additional libraries that we are going to use in this notebook are: \n",
    " - [gensim](https://pypi.org/project/gensim/) -  Python library for topic modelling\n",
    " - [nltk](https://www.nltk.org/) - natural language toolkit, library to work with language\n",
    " - [pyLDAvis](https://pypi.org/project/pyLDAvis/) - library for interactive topic model visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    !pip install  --user  urllib\n",
    "    import urllib.request\n",
    "    \n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    !pip install  --user  pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import gensim\n",
    "    import gensim.corpora as corpora\n",
    "    from gensim.utils import simple_preprocess\n",
    "except:\n",
    "    !pip install  --user  gensim\n",
    "    import gensim\n",
    "    import gensim.corpora as corpora\n",
    "    from gensim.utils import simple_preprocess\n",
    "    \n",
    "try:\n",
    "    import pyLDAvis.gensim\n",
    "except:\n",
    "    !pip install  --user pyldavis\n",
    "    import pyLDAvis.gensim\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn\n",
    "except:\n",
    "    !pip install  --user  nltk\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the cleaned dataset from object store and display first 5 rows\n",
    "There is a copy of cleaned dataset saved to object store. You can download it or use your local copy, created in the second notebook.\n",
    "We reading csv file into pandas dataframe and printing first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_url=\"https://swift-yeg.cloud.cybera.ca:8080/v1/AUTH_233e84cd313945c992b4b585f7b9125d/geeky-summit/tweets_cleaned1.csv\"\n",
    "file_name=\"tweets_cleaned1.csv\"\n",
    "urllib.request.urlretrieve(target_url, file_name) ## comment out this line to use your local copy \n",
    "\n",
    "tweets = pd.read_csv(file_name,parse_dates=['created_at_date'])  ## reading 'created_at_date' column as timestamp\n",
    "pd.set_option('max_colwidth', 20)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 Subsetting the data\n",
    "To make LDA algoritm work faster we will use only subset of the data instead of the entire dataset.  \n",
    "Let's try subsetting by the day first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_subset_nov5=tweets.loc[tweets[\"created_at_date\"].dt.day==5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2 Preparing data for LDA\n",
    "\n",
    "#### [Tokenization](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html) is the task of chopping sentence up into pieces, called tokens, (in our case we will split tweets into words).  \n",
    "First we transform extended_tweet_cleaned column into a List of Strings (tweets).   \n",
    "We are only going to be working with this column  - so don't need the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=tweets_subset_nov5[\"extended_tweet_cleaned\"].tolist()\n",
    "print (data[1]) ###printing 1st element of the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we will go through every String(tweet) in  the List and transform it into another List: list of words(tokens).  \n",
    "We are going to use [gensim.utils.simple_preprocess](https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess) function, it converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        ## will ignore words shorter than 4 and longer than 15 characters\n",
    "        yield(gensim.utils.simple_preprocess(sentence,min_len=4, max_len=15))  \n",
    "\n",
    "data_tokens = list(string_to_words(data))\n",
    "\n",
    "print(data_tokens[1]) ###printing 1st element of the list after tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing [stowords](https://pythonspot.com/nltk-stop-words/) - removing words that don't add any value and can be excluded from ananlysis\n",
    "NLTK libabry has some predefined stopwords, we will download them and examine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#stop_words.add('calgary')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)  - reducing word forms and unifying them to one main form.\n",
    "For example:\n",
    ">is $\\Rightarrow$ be   \n",
    ">car, cars $\\Rightarrow$ car\n",
    "\n",
    "We are going to use wn.morphy() function to get lemmas for words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_lemma(\"is\"))\n",
    "print(get_lemma(\"cars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tokens_to_lda(data_tokens):\n",
    "    for data_token in data_tokens:\n",
    "        tokens = [token for token in data_token if token not in stop_words]\n",
    "        tokens = [get_lemma(token) for token in tokens]\n",
    "        yield tokens\n",
    "tokens = list(tokens_to_lda(data_tokens))\n",
    "\n",
    "###printing 1st element of the list after removing stopwords and lemmatization\n",
    "print(tokens[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dictionary and corpus objects for LDA model\n",
    "We will create dictionary and corpus for LDA model.  \n",
    "\n",
    "**[Dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary)** - a mapping between words and their integer ids.  \n",
    "**Corpus** -  list of (word_id, word_frequency) for every document.\n",
    "\n",
    "We will be  using [corpora.Dictionary()](https://radimrehurek.com/gensim/corpora/dictionary.html)  and \n",
    "[doc2bow()](https://kite.com/python/docs/gensim.corpora.dictionary.Dictionary.doc2bow) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokens]\n",
    "\n",
    "print(\"First 10 words in the dictionary: \", list(dictionary.token2id)[:10])\n",
    "\n",
    "print(\"First tweet, tokens: \",tokens[1])\n",
    "print(\"First tweet, corpus: \",corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Building LDA model with 7 topics and displaying top 8 words for every topic\n",
    "\n",
    "We are going to use [gensim.models.ldamodel.LdaModel()](https://radimrehurek.com/gensim/models/ldamodel.html) function with predefined number of topics - 8 and print top 7 words in every topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=7, \n",
    "                                           random_state=100)\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=8)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Visualizing the model \n",
    "We are going to use [pyLDAvis.gensim.prepare] (https://pyldavis.readthedocs.io/en/latest/modules/API.html#pyLDAvis.prepare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary,sort_topics=True)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  exercise: try different days, try modifying stop words, number of topics  and min_len /max_len in `gensim.utils.simple_preprocess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduced the idea of topic modeling on a corpus of text, and showed the results of a popular topic modeling algorithm, namely LDA. We also outlined some important pre-processing steps that one can take in order to prepare text data for analysis in a topic model. We outlined tokenization, lemmatization and the removal of stop words in order to simplify our corpus in order to simplify the corpus for our topic model. This notebook should prepare you for the next notebook which combines sentiment analysis and topic modeling to understand how people \"feel\" about a given topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
