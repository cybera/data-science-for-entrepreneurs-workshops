{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Twitter data\n",
    "In this notebook, we are going to explore our cleaned dataset that we created in the [first notebook.](1.Data_wrangling_and_cleaning.ipynb)\n",
    "\n",
    "[Exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. (Wikipedia)\n",
    "\n",
    "We are going to do exploratory analysis in order to understand the shape of the data, patterns and values, correlations between features, and hidden meaning behind our data.\n",
    "\n",
    "## Goal: \n",
    "Learn some common aspects of data exploration, calculate some statistics and visualize data column by column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load python modules\n",
    "\n",
    "There are many packages available in Python that provide a variety of functions for data science. Here we load the libraries needed for this notebook.\n",
    "\n",
    " - [wordcloud](https://amueller.github.io/word_cloud/) -  used to create wordclouds in Python\n",
    " - [nltk](https://www.nltk.org/) - natural language toolkit, library to work with language.\n",
    " - [folium](https://python-visualization.github.io/folium/) - library for creating maps.\n",
    " \n",
    "We'll also be using [pandas](https://pandas.pydata.org/) to work with DataFrames, one of the most common libraries used for data science. There are good tutorials available on pandas and worthwhile spending more time with in order to become more comfortable with data science in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "import pandas as pd   #will reference pandas as pd\n",
    "\n",
    "import nltk\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt #will reference matplotlib as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from importlib import reload\n",
    "import site\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we will read the cleaned dataset from helper git repo and display the first 5 rows\n",
    "\n",
    "\n",
    "We will read the csv file into pandas DataFrame and print the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name=\"../data/2020-01-20_start_cleaned_shortened.csv\"\n",
    "\n",
    "# reading 'created_at_date' column as timestamp\n",
    "tweets_cleaned = pd.read_csv(file_name, parse_dates=['created_at_date']) \n",
    "\n",
    "#fix timezone\n",
    "tweets_cleaned['created_at_date']=tweets_cleaned['created_at_date'].dt.tz_convert('MST')\n",
    "\n",
    "tweets_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Intro to pandas DataFrames\n",
    "<details>\n",
    "<summary> Click here to learn some basic operations on pandas DataFrames </summary>\n",
    "<br>\n",
    "\n",
    "`tweets_cleaned` is a DataFrame - 2dimentional data structure(similar to a table). We work with DataFrames in Python using pandas python library.    \n",
    "Note, there are a few different ways for accessing data inside a DataFrame. We'll cover a couple of aspects here, but you may also want to read more in the pandas documentation([here](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python) and \n",
    "[here](https://cmdlinetips.com/2018/02/how-to-subset-pandas-dataframe-based-on-values-of-a-column/)) or cheat sheet such as the one handed out [here](https://www.dataquest.io/blog/large_files/pandas-cheat-sheet.pdf). \n",
    "\n",
    "<h4> Basic operations with DataFrames</h4>\n",
    "\n",
    "<h5>Display on the screen</h5>\n",
    "\n",
    "**head()** - without arguments displays first 5 rows, or takes number of rows to display as an argument:    \n",
    "\n",
    "```python\n",
    "tweets_cleaned.head()   \n",
    "tweets_cleaned.head(10)\n",
    "```\n",
    "\n",
    "DataFrame name  will display the entire dataframe:   \n",
    "```python\n",
    "tweets_cleaned\n",
    "```\n",
    "\n",
    "Note, you might need to use **print()** function if there are multiple dataframes displayed in one code cell:  \n",
    "```python\n",
    "print(tweets_cleaned.head())\n",
    "```\n",
    "\n",
    "<h5>Select set of rows/colums</h5>\n",
    "\n",
    "Selecting one column:   \n",
    "```python\n",
    "tweets_cleaned['created_at_date']\n",
    "```\n",
    "\n",
    "Selecting multiple columns:  \n",
    "```python\n",
    "tweets_cleaned[['created_at_date','extended_tweet_cleaned']]  \n",
    "```\n",
    "\n",
    "Selecting one row(row 2):  \n",
    "```python\n",
    "tweets_cleaned.iloc[[2]]\n",
    "```  \n",
    "\n",
    "Selecting multiple rows(rows 2 and 5):   \n",
    "```python\n",
    "tweets_cleaned.iloc[[2,5]]\n",
    "``` \n",
    "\n",
    "Selecting range of rows(rows 2,3,4 and 5):   \n",
    "```python\n",
    "tweets_cleaned.iloc[[2:5]]\n",
    "```\n",
    "\n",
    "Selecting rows and columns:   \n",
    "```python\n",
    "tweets_cleaned[['created_at_date','extended_tweet_cleaned']].iloc[[2,5]]\n",
    "```  \n",
    "\n",
    "Note, row numbers start at 0.\n",
    "   \n",
    "<h5>Subset by value in specific column</h5>\n",
    " \n",
    "First we need to create a boolean expression for a subset(let's say we want to filter out rows where value of `user_location` is equal to `Calgary,AB` ):   \n",
    "\n",
    "```python\n",
    "bool_expression= tweets_cleaned['user_location']==\"Calgary,AB\"\n",
    "```\n",
    "\n",
    "\n",
    "**Boolean expression**  in this case is an expression that can be either **True** or **False** for every line in DataFrame, for example:   \n",
    "```python\n",
    "0        False  \n",
    "1        False  \n",
    "2        False  \n",
    "3        False  \n",
    "4        True  \n",
    "5        True  \n",
    "6        True  \n",
    "7        False  \n",
    "8        False  \n",
    "9        False  \n",
    "10       True  \n",
    "...\n",
    "```\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Subset DataFrame by boolean expression:   \n",
    "```python\n",
    "tweets_cleaned[bool_expression].head(10)\n",
    "```\n",
    "\n",
    "**Other  examples of boolean expressions**:\n",
    "\n",
    "Not equal:   \n",
    "```python\n",
    "bool_expression = tweets_cleaned['user_location']!=\"Calgary,AB\"\n",
    "```\n",
    "\n",
    "Is part of list of values:\n",
    "```python\n",
    "bool_expression = tweets_cleaned['user_location'].isin([\"Calgary,AB\",\"Canada\"])\n",
    "```\n",
    "\n",
    "Multiple conditions  - \"and\":\n",
    "```python\n",
    "bool_expression = (tweets_cleaned['user_location']==\"Calgary,AB\") & (tweets_cleaned['name']!=\"Dave\")\n",
    "```\n",
    "\n",
    "Multiple conditions - \"or\":  \n",
    "```python\n",
    "bool_expression = (tweets_cleaned['user_location']==\"Calgary,AB\") | (tweets_cleaned['name']!=\"Dave\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "#### We will explore the data column by column \n",
    "A couple of things to keep in mind while doing this is what are we expecting from the data? Are our observations consistent with these expectations? If not, why do they not line up? Are there any trends, outliers, or interesting observations to make note of? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the column names?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `extended_tweet_cleaned` column\n",
    "\n",
    "Let's begin with this column as this is likely the most interesting one to us. It contains all the text data tweeted out in each tweet. We would like to analyze this further. One way to examine the tweets is by looking at word frequencies. This could be done by using a bar chart or alternatively, using a word cloud. \n",
    "\n",
    "As you might imagine, if  we just go ahead and create a word cloud, it will be dominated by some very commonly used words, such as \"the\", \"or\", \"and\", etc. In order to prevent these common words from dominating the plot, we will remove them. Commonly used words that don't add anything contextually interesting such as this are called \"stopwords\". The wordcloud package that we will be using has a pre-built list of stopwords. \n",
    "\n",
    "Let's import them and examine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import  STOPWORDS\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the [WordCloud()](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud) function from the wordcloud library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "#In the following function, random_state is set for reproducibility and collocation=False means ignore bigrams (two words combinations)\n",
    "def wordcloud(tweets,col):\n",
    "    words=\" \".join([i for i in tweets[col]]) # we join all tweets in one character string \n",
    "    wordcloud = WordCloud(width=800, height=400,collocations=False,background_color=\"white\",stopwords=stopwords,random_state = 2019).generate(words)\n",
    "    plt.figure(figsize=(20,10)) #size of the wordcloud\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(tweets_cleaned,'extended_tweet_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All these words make a lot of sense, except for... amp?? Let's find some sample tweets that will allows us to inspect them why people might be tweeting about amps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 700) #make the output wider to be able to see more text\n",
    "rows_containing_amp=tweets_cleaned['extended_tweet_cleaned'].str.contains(\"amp\")\n",
    "tweets_cleaned[rows_containing_amp]['extended_tweet_cleaned'].head(10)  #displaying first 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that &amp is just a shortcut for ampersand. This is a bit misleading and so let's delete all the &amp occurences from the 'extended_tweet_cleaned' column. \n",
    "\n",
    "What would be another way of dealing with the confusing &amp text? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned['extended_tweet_cleaned']=tweets_cleaned['extended_tweet_cleaned'].str.replace('&amp',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclude \"one\" and \"will\" words\n",
    "\"one\" and \"will\" words can possibly be excluded as well, they don't have any special meaning. In this case, we'll add them to the list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.add(\"one\")\n",
    "stopwords.add(\"will\")\n",
    "wordcloud(tweets_cleaned,'extended_tweet_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise1: Add Calgary and AB (or other words as you like) to stopwords to exclude them from the wordcloud and try plotting the wordcloud again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### type your code here\n",
    "\n",
    "\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - this word cloud looks more sensible. What picture emerges from this? Are there any  themes we can identify? People seem to be thankful, something about time - maybe mentioning a good time? While we can guess at some of these aspects, it's impossible to say anything about the context these words occur in. For that, we would need some more sophisticated analyses. This could include aspects like n-gram analysis or topic modelling, which is what we will take a look at in the next notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Created_at_date` column\n",
    "This column is of type Timestamp. \n",
    "The following [features](https://www.geeksforgeeks.org/python-working-with-date-and-time-using-pandas/) can be useful for working with the Timestamp format:\n",
    " - **dt.year** returns the year of the date time.\n",
    " - **dt.month** returns the month of the date time.\n",
    " - **dt.day** returns the day of the date time.\n",
    " - **dt.hour** returns the hour of the date time.\n",
    " - **dt.minute** returns the minute of the date time.\n",
    "   \n",
    " **min(), max()** functions can be used with timestamp as well.   \n",
    "   \n",
    " Let's find out the time range first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Time range: \",min(tweets_cleaned[\"created_at_date\"]),\"-\",max(tweets_cleaned[\"created_at_date\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Let's group by day and calculate the number of tweets for each day.\n",
    "\n",
    "We will use the [groupby](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) function, which is a pandas function that groups rows into groups based on one or multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by day\n",
    "tweets_cleaned_by_day=tweets_cleaned.groupby(tweets_cleaned[\"created_at_date\"].dt.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##size of the plot is 5/5 square to make it circle, not oval \n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#Select the 'created_at_date' column from grouped dataframe, count up, and plot\n",
    "tweets_cleaned_by_day[\"created_at_date\"].count().plot(kind=\"pie\") #making pie chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pie chart shows the proportion of tweets per day. The number on the circle corresponds to the day of the month the data was collected. \n",
    "\n",
    "Another way to plot this is as a bar chart: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)) ## making bigger plot rectangular shape in this case\n",
    "\n",
    "tweets_cleaned_by_day[\"created_at_date\"].count().plot(kind=\"bar\") #making bar plot\n",
    "\n",
    "plt.xlabel(\"Day\", size =16)\n",
    "plt.ylabel(\"Counts\", size =16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the most tweets took place on January 20th. Let's confirm that and check out some additional summary stats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape[0] - gives number of rows, shape[1] - number of columns\n",
    "\n",
    "print(\"Total number of tweets: \",\n",
    "      tweets_cleaned.shape[0])\n",
    "\n",
    "#subsetting only rows where day is equal to 20\n",
    "bool_exp_day_20 = tweets_cleaned[\"created_at_date\"].dt.day==20\n",
    "\n",
    "print(\"Number of tweets collected on 20th: \", \n",
    "      tweets_cleaned[bool_exp_day_20].shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at the number of tweets by hour of day and whether this make sense with what we expect. What kind of pattern would you expect? \n",
    " \n",
    "#### Exercise2: Using the same methodology as for the bar chart above, can you plot the number of tweets by hour? \n",
    "Note this is a summary plot for the same hour for all days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type your code here\n",
    "\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by hour\n",
    "tweets_cleaned_by_hour=tweets_cleaned.groupby(tweets_cleaned[\"created_at_date\"].dt.hour)\n",
    "\n",
    "#selecting hour 3 to get data between 3:00 and 4:00\n",
    "print(\"Total number of tweets collected between 03:00 and 04:00: \",\n",
    "      tweets_cleaned_by_hour[\"created_at_date\"].count()[3])\n",
    "\n",
    "#selecting hour 18 to get data between 18:00 and 19:00\n",
    "print(\"Total number of tweets collected between 18:00 and 19:00: \", \n",
    "      tweets_cleaned_by_hour[\"created_at_date\"].count()[18])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise3: plot tweets number by day and hour \n",
    "**Hint1**:   \n",
    "For two values in groupby() function:\n",
    ">instead of `groupby([value])` use `groupby([value1,value2])` \n",
    "\n",
    "Note how plot can be different if you do `groupby([day,hour])`  vs `groupby([hour,day])`.\n",
    "\n",
    "**Hint2**: \n",
    ">Use `plt.figure(figsize=(20,10))` to make larger plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## type your code here\n",
    "\n",
    "\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - so far everything looks as we expected it for when the tweets were collected and at which time of day users were  most active. Let's move on to the next column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Hashtag_string`  and `user_string` columns\n",
    "These two columns are of type String and have a list of hastags/user mentions separated by a blank.  \n",
    "We will create a list of all the hashtags first using the [join()](https://www.tutorialspoint.com/python/string_join.htm) and [split()](https://www.w3schools.com/python/ref_string_split.asp) functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags=\" \".join([i for i in tweets_cleaned['hashtags_string']]) #join all the hashtags into one character sting\n",
    "all_hashtags=all_hashtags.split()  # transform into a list\n",
    "print(all_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the [FreqDist()](https://kite.com/python/docs/nltk.probability.FreqDist) function from the nltk library to get frequency distributions for all the words.  That is, this function will count how many times each hashtag occurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freque_dist=nltk.FreqDist(all_hashtags)\n",
    "print(\"Most common hashtags: \",freque_dist.most_common(20)) ## most_common(n) function prints top n words with highest frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize this using matplotlib to print the top 25 most common hashtags. It's possible to use the [plot()](https://kite.com/python/docs/nltk.probability.FreqDist.plot) function directly with a  FreqDist object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)) # plot size\n",
    "\n",
    "plt.title(\"Most common hashtags\", size = 20) #20 is the  size of the font in this case\n",
    "plt.xlabel(\"Hashtag\", size = 16)\n",
    "plt.ylabel(\"Counts\", size = 16)\n",
    "plt.xticks(fontsize=14) ## Change font for x axis labels (actual hashtags)\n",
    "freque_dist.plot(25) # plotting top 25 most frequent hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same steps for the user_string column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_users=\" \".join([i for i in tweets_cleaned['user_string']])\n",
    "all_users=all_users.split()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Most common user mentions\", size=20)\n",
    "plt.xlabel(\"Tagged User\", size=16)\n",
    "plt.ylabel(\"Number of tags\", size =16)\n",
    "plt.xticks(fontsize=14)\n",
    "fd = nltk.FreqDist(all_users)\n",
    "fd.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise4 - try subsetting the data by hour (for example between 8  and 9 pm) and plot the most common hashtags or user_mentions\n",
    "Hint: to select only data between 8 and 9 pm use the dt.hour==20 boolean expression:\n",
    ">bool_expression_hour=tweets_cleaned[\"created_at_date\"].dt.hour==20   \n",
    ">tweets_cleaned[bool_expression_hour]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type your code here\n",
    "\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many users talked about the most commonly mentioned user? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_user1 = nltk.FreqDist(all_users).most_common(1)[0][0]\n",
    "\n",
    "print(most_common_user1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [str.contains()](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.Series.str.contains.html) function to get rows containing specific string:\n",
    "> tweets_cleaned['user_string'].str.contains(most_common_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_expr=tweets_cleaned['user_string'].str.contains(most_common_user1)\n",
    "\n",
    "print(\"Number of tweets with the most common user mention \" +most_common_user1,\n",
    "      len(tweets_cleaned[bool_expr]['name']))\n",
    "\n",
    "print(\"Number of users using the most common user mention \"+ most_common_user1,\n",
    "      len(tweets_cleaned[bool_expr]['name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare, how does this compare to the second most commonly mentioned user? Check the bar chart above and find out how many users mentioned it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_user2 = nltk.FreqDist(all_users).most_common(2)[1][0]\n",
    "\n",
    "print(most_common_user2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise5 - How many users talked about the second most commonly mentioned user? \n",
    "\n",
    "Hint: use coe above as an example, replace most_common_user1 with most_common_user2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## type your code here\n",
    "\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Who had the most number of user mentions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user mentiones for every tweet are stored as a character string where single mentions are separated by space\n",
    "\n",
    "#str.split() transforms string into a list,\n",
    "#str.len() calculates the lengths of the list - how many user mentiones per tweet\n",
    "#max() - finds the max number of user mentiones\n",
    "\n",
    "max_user_mentions=tweets_cleaned['user_string'].str.split().str.len().max()\n",
    "\n",
    "rows_with_max_user_mentions=tweets_cleaned['user_string'].str.split().str.len()==max_user_mentions\n",
    "\n",
    "print(\"Maximum number of user mentions:\",\n",
    "      max_user_mentions,\n",
    "      \"made by  \",\n",
    "      tweets_cleaned[rows_with_max_user_mentions] ['name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### `User_location` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations=tweets_cleaned[\"user_location\"].unique()\n",
    "\n",
    "print(\"Number of unique user locations:\", len(unique_locations), \"\\n\")\n",
    "print(unique_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more than 1000 unique user_locations. These are locations that are entered by users in the profile and open to pretty much any input, which is why we see some odd locations in there. Not sure if this is overly interesting right now. Let's look into latitude/longitude instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Latitude/longitude` columns\n",
    "We will plot tweets that have coordinates on a map.\n",
    "\n",
    "How are latitude/longitude added to a tweet? Do all tweets have them? How reliable is this data source? A few things to keep in mind as we go through this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of tweets: \",tweets_cleaned.shape[0])\n",
    "\n",
    "rows_with_location=tweets_cleaned[\"longitude\"].notnull() #notnull() function checks if the element is not NaN\n",
    "tweets_have_location=tweets_cleaned[rows_with_location]\n",
    "\n",
    "print(\"Number of tweets having location data: \",tweets_have_location.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the location data on a map and use the library [folium](https://github.com/python-visualization/folium). We will iterate through each row  of data and add the coordinates to the map where they exist. \n",
    "\n",
    "This can be accomplished using the [iterrrows()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html) function to iterate through dataframe rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "calgary_coords = [51.0486, -114.0708] # Calgary coordinates, (where we are going to center the map)\n",
    "my_map = folium.Map(location = calgary_coords, zoom_start = 5)\n",
    "\n",
    "#we use head(500) to reduce the number of points to 500 to make map work faster, \n",
    "#try playing with the number of points to display or remove head() to display all points\n",
    "subset=tweets_have_location.head(500)\n",
    "\n",
    "for index,row in subset.iterrows(): \n",
    "        folium.Marker([row[\"longitude\"], row[\"latitude\"]]).add_to(my_map) \n",
    "my_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the most common user mention for tweets having location data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users=\" \".join([i for i in tweets_have_location['user_string']]).split()\n",
    "user_mention_location_data = nltk.FreqDist(all_users).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "user_mention_location_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think is MHC_Rattlers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise6: try subsetting by  the most common user mention(user_mention_location_data) and  plot tweets on a map\n",
    "Hint: use str.contains() function applied to tweets_have_location['user_string'] colum:\n",
    ">bool_expression_usermentions=tweets_have_location['user_string'].str.contains(\"your_search_word\")\n",
    ">tweets_have_location[bool_expression_usermentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### type your code here\n",
    "\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Screen_name` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the user name and using it we can find the most active users(users having the most number of tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Total number of tweets: \",tweets_cleaned.shape[0])\n",
    "\n",
    "print(\"Total number of users: \",tweets_cleaned['screen_name'].unique().shape[0])\n",
    "\n",
    "print(\"Top 5 most active users:\")\n",
    "\n",
    "tweets_grouped_by_screenname=tweets_cleaned[\"screen_name\"].groupby(tweets_cleaned[\"screen_name\"])\n",
    "\n",
    "tweets_grouped_by_screenname.count().reset_index(name='count').sort_values(['count'], ascending=False).head(5)#.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `In_reply_to_status_id_str` and `Extended_tweet_cleaned_thread` columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`in_reply_to_screen_name` and `extended_tweet_cleaned_threa`' columns have information about the author of the original thread and the first tweet  in the original thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned[['in_reply_to_screen_name',  'extended_tweet_cleaned_thread']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same method as above  - lets find out the top 5 the most active threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 5 most active threads:\")\n",
    "\n",
    "tweets_grouped_by_thread=tweets_cleaned['extended_tweet_cleaned_thread'].groupby(tweets_cleaned['extended_tweet_cleaned_thread'])\n",
    "\n",
    "tweets_grouped_by_thread.count().reset_index(name='count').sort_values(['count'], ascending=False).head(5)#.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion\n",
    "\n",
    "We've done some data exploration in order to try to understand the data better. Our time spent on each column depended on how complex the data is each column. Since we will continue to work with the text data, that is where we started and spent most of our time working with. \n",
    "\n",
    "Take-aways: \n",
    "* Data exploration helps provide a sense of the data that is there and often reveals interesting trends and patterns \n",
    "* Always consider both exploring the data using summary statistics along with data visualizations\n",
    "* Think about what you would expect the data to look like going into it and determine whether your assumptions hold up. If not, why not? This could tell you important things about your data. \n",
    "* Subject matter expertise is invaluable in data science, including in the exploration phase. A solid understanding of how Twitter is used, or working with someone who does, can often quickly resolve questions that could otherwise take hours to answer. \n",
    "\n",
    "Next, we will go deeper into natural language processing, will  build topic models  and do sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.add(\"Calgary\")\n",
    "stopwords.add(\"Alberta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by hour\n",
    "tweets_cleaned_by_hour=tweets_cleaned.groupby(tweets_cleaned[\"created_at_date\"].dt.hour)\n",
    "\n",
    "#Select the 'created_at_date' column from grouped dataframe count up, and plot\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "tweets_cleaned_by_hour[\"created_at_date\"].count().plot(kind=\"bar\")\n",
    "plt.xlabel(\"Hour\", size =16)\n",
    "plt.ylabel(\"Counts\", size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned_by_hour_day = tweets_cleaned.groupby([tweets_cleaned[\"created_at_date\"].dt.day,tweets_cleaned[\"created_at_date\"].dt.hour])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "tweets_cleaned_by_hour_day[\"created_at_date\"].count().plot(kind=\"bar\", figsize=(20,10))\n",
    "\n",
    "plt.xlabel(\"Day/Hour\", size =16)\n",
    "plt.ylabel(\"Counts\", size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_expression_hour=tweets_cleaned[\"created_at_date\"].dt.hour==20\n",
    "tweets_cleaned_20=tweets_cleaned[bool_expression_hour]\n",
    "\n",
    "all_users=\" \".join([i for i in tweets_have_location['user_string']])\n",
    "all_users=all_users.split()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Most common user mentions between 20 and 21\", size=20)\n",
    "plt.xlabel(\"Tagged User\", size=16)\n",
    "plt.ylabel(\"Number of tags\", size =16)\n",
    "plt.xticks(fontsize=14)\n",
    "fd = nltk.FreqDist(all_users)\n",
    "fd.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_expr=tweets_cleaned['user_string'].str.contains(most_common_user2)\n",
    "\n",
    "print(\"Number of tweets with the most common user mention \" +most_common_user2,\n",
    "      len(tweets_cleaned[bool_expr]['name']))\n",
    "\n",
    "print(\"Number of users using the most common user mention \" +most_common_user2,\n",
    "      len(tweets_cleaned[bool_expr]['name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_expression_usermentions=tweets_have_location['user_string'].str.contains(user_mention_location_data) \n",
    "tweets_subset_library=tweets_have_location[bool_expression_usermentions]\n",
    "\n",
    "my_map = folium.Map(location = calgary_coords, zoom_start = 5)\n",
    "\n",
    "for index,row in tweets_subset_library.iterrows():  \n",
    "        folium.Marker([row[\"longitude\"], row[\"latitude\"]]).add_to(my_map) \n",
    "my_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
